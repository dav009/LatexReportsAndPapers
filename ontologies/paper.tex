%\documentclass[4pt,a4paper]{article}
\documentclass[4pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{algorithm}
\usepackage{textcomp}
\usepackage{fontenc}
\usepackage{tipa}
\usepackage{framed}
\usepackage{multicol}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pbox}
\usepackage{cite}
\usepackage{lastpage}
\usepackage{lmodern}

%\usepackage{setspace}
%\doublespacing


\usepackage[]{hyperref}
\hypersetup{  
	colorlinks=true,
    urlcolor=cyan           % color of external links
}

\author{David Przybilla\\dav.alejandro@gmail.com, davida@coli.uni-saarland.de\\ \\ Term Paper for Knowledge Representation Seminar\\ Universit\"{a}t des Saarlandes}
\title{Automatic Generation of Domain Models}
\begin{document}
\twocolumn[
	 \begin{@twocolumnfalse}
    \maketitle
  \end{@twocolumnfalse}
 ]




\section{Introduction}


\begin{figure*}[]
  \centering
    \includegraphics[scale=0.2]{pics/problem.jpg}
    \caption{Problem Diagram}
   \label{fig:problem}  
\end{figure*}



This report focus on the paper \textit{Automatic Generation of Domains Models for call centers from noisy transcriptions} ~\cite{Roy:2006:AGD:1220175.1220268}.\\
In the given paper the authors describe a method for automatically creating a taxonomy.\\
The domain of the problem are the issues of a Callcenter. The Call center handles user problems related to different software and services.\\
The proposed method use  written and speech records between the agents of the call center and the clients to built the ontology. \\
In the next sections  the method for transforming unstructured data into a knowledge representation proposed by the paper will be described, a critical review and opinion will be given at the final paragraph of each subsection. Additionally a final section compares the given paper and other papers willing to automatically construct knowledge representation as well as discussing possible applications in the NLP area.


\section{Motivation}

Automatically creating knowledge representation from unstructured text has been a focus of study given the amount of data available in the web.
Projects such as 'Know it all'\footnote{\url{http://www.cs.washington.edu/research/knowitall/}} considers the problem of developing a variety of domain-independent systems that extract information from the Web in an autonomous, scalable way.\\ Google\footnote{\url{http://www.google.com/insidesearch/features/search/knowledge.html}} for example has the project 'The knowledge Graph' which considers the automatically construction of an ontology by using the web knowledge. \\
\\
Nevertheless the unstructured nature of the information has proven to be an challenge when constructing knowledge representations, natural languages arises many problems when trying to understand their semantics and to abstract concrete cases into general relations and entities.\\
In the opinion of this reviewer Information Extraction provided a first simple task into automatically creating structures from natural language texts. The task in information extraction consist in gathering  relations among entities from raw text.\\
In the first stages of this task only a very specific range of relations and entities are to be caught.
For example one would like to gather all relations and entities talking about  "getting the Nobel prize" or "change of chairs in a company".\\
However in the recent years this trend has evolved towards "Open Information Extraction", which wants to capture as much as entities and relations as possible given raw text.\\
The applications to information extraction are countless, the output of the task provide a resource for other more sophisticated processes, and it could be useful for example to topic mining, opinion mining or information retrieval or to correlate different relations or events captured from different sources.
In the opinion of the reviewer this provides a first step into building an ontology, since it would allow initially to have a candidate pool of concepts and relations.\\
\\
During the last years focus has been put into strategies which allow to capture structures in raw text, for example Cortez and Oliveira ~\cite{Cortez:2011:JUS:1989323.1989380} proposed a method for extracting data from semi-structured texts. In their experiments their system learns without human intervention the structures in which Cooking recipes are written and it is able to match each step and ingredients of the recipes into a structured representation.\\
This could be a major improvement since it would allow for example to capture entities and relations from soruces of knowledge that are semi-structured like Wikipedia.\\
\\
Additionally The reviewer would like to mention that even though there is a focus on creating Ontologies, there is also a focus on Extending the current ones in an automatic way.\\
Many tasks require Knowledge sources in order to achieve good performance, however this depends deeply on the coverage of the Knowledge representation. 
Consider for example the applications using Wordnet or Framenet, they heavily rely on the coverage of those sources to disambiguate the senses of the words.
In ~\cite{Tonelli:2013:WWM:2405838.2405917} Tonelli proposes a method for extending this Knowledge Sources by using Wikipedia in a semi-automatic way.
Following the same trend Yamada ~\cite{yamada-EtAl:2011:IJCNLP-2011} tries to extend Wordnet in an automatic way by using Wikipedia.\\
Following a different direction Pinkal,Regneri and Koller  in ~\cite{regneri-koller-pinkal:2010:ACL} propose the use of crowdsourcing to gather scripts of common activities (like shopping, cooking eggs) in order to model $Common\, Knowledge$ which is often overlooked by computer systems.


\section{Preprocessing }
As an initial step to start the process of creating the taxonomy a preprocessing step is proposed by the paper.\\
In the preprocessing two major steps take place, first the Speech Data is transformed into text and second the text is transformed into feature vectors for the method in charged of building the taxonomy.



\subsection{Automatic Speech Recognition}
		
During this step the conversations between agents and clients which have been recorded are transformed into text using Automatic Speech Recognition.\\
The authors of the paper mention that they transformed more than 2000 calls  but they never mention the ratio of already given transcriptions to the generated transcriptions using Speech Recognition.\\
The authors also describe several issues that they identified for this step, they actually claim that the data generated during this process is very noisy with an error rate of about 40\%.\\
Some of the issues are:
\begin{itemize}

	\item Different Accents: Since the call center takes calls from all over the world, the speakers usually have different accents. The tool they used for ASR seems to be affected by this fact.
	
	\item Deletion of Words 
	
	\item Wrong words are inserted
	
	\item Wrong Speaker is assigned
	
	\item no punctuation marks
	
	\item silence periods
	
	\item no sentence boundaries
	
	\item false starts
	
	\item filling words (i.e : "um", "uhh")
\end{itemize}

For the reviewer of this paper is unclear whether this are actually issues.\\
First there is no clear view of the proportion betweet transcript data and the data generated by the speech recognition step, that is, if the proportion is very small it would not matter at all.\\
Secondly given the BoW(Bag of Words) approach they use in their system later, is irrelevant that the wrong speaker is assigned, or whether there is punctuation marks or not. Moreover I would speculate that having wrong words inserted or deletions of words should not be so painful for the performance of their system. I will explain this phenomena by the end of the next subsection.


\subsection{Feature Engineering component}

This corresponds to the second step of the preprocessing. In this step the texts are pruned and transformed into feature vectors (Figure ~\ref{fig:eng_component} ).\\
Each document pass through the following filters:

\begin{figure*}[]
  \centering
    \includegraphics[scale=0.2]{pics/eng_component.jpg}
    \caption{Feature Engenieering component}
   \label{fig:eng_component}  
\end{figure*}

\begin{itemize}
	\item Stop Word Removal: Functional words which do not convey relevant meaning are removed (i.e: ''for",''of"..).
	
	\item stemmer: Each word is converted into its stem. This has the objective of aligning dimensions in the following process for building the taxonomy.
	
	\item extract ngrams:  generate a feature vector of each document with Ngrams.
\end{itemize}

For the reviewer all of the filters mentioned should work right given that the taxonomy builders require text clustering.\\
Nevertheless the previous mentioned noisy inclusion by the ASR system could be objected as follows:

\begin{itemize}
	\item Conversations are long enough. Conversations of this kinds are long enough for keywords to be repeated several times during the conversations, that is, even if there is word deletion the system would have several opportunities to capture the right keywords, the other words which are not likely to be keywords will be discarded by the system anyway.

  \item Given the BoW approach, the wrong word insertion would only harm when they inserted word which is a domain keyword.
  
\end{itemize}

For the reviewer further steps could be done given that the creation of the taxonomy relies on Text clustering.\\
It should be possible for example to extend each document with extra keywords in order to have a better dimension aligment.This have been done in solving tasks such as  textual entailment ~\cite{Mirkin:2009:EIU:1609067.1609129} where hypothesis have been extended using Wordnet,framenet and other sources of knowledge. This strategy has also been used in text clustering of short texts (i.e: tweets)  in order to gain coverage and recall, in ~\cite{Gabrilovich:2006:OBB:1597348.1597395} Gabrilovich proposes a strategy for extending tweets with words from Wikipedia.\\
The call center could for example use keywords from the manual of the software they support.


\section{Creating the Taxonomy}

Once each document is transformed into a vectorial space representation the component in charged of creating the taxonomy takes place. The Taxonomy Component uses text clustering to find a hierarchy of Topics.\\
Each of the following subsections describes in more detail what happens during this process.

\subsection{Clusterer}
The main idea of this step is to use a bisection clustering algorithm many times to have different clusters the documents.\\
The bisection algorithm receives as an input the number of clusters ($K$) and the documents. It progressively looks for clusters until the $K$ clusters are found.
\begin{figure*}[]
  \centering
    \includegraphics[scale=0.2]{pics/clusterer2.jpg}
    \caption{Clusterer}
   \label{fig:clusterer}  
\end{figure*}

This component runs the bisection algorithm with many different $K$. The underlying assumption is that the clustering results where $K$ is a high number will refer to a more fine granularity of topics, whereas a clustering with a lower value of $K$ will result in a higher level topic granularity (Figure ~\ref{fig:clusterer}).\\
In other words, if a topic were for example operative systems, then there would be a cluster with a low value of $K$ which will cover this topic, and the clusters covering the topics MacOS and Windows will be generated by a run with a much higher $K$.\\
\\
The result by the end of this step is a set of clusters, the authors assume that each cluster refers to a topic.

\subsection{Taxonomy Builder}

\begin{figure*}[]
  \centering
    \includegraphics[scale=0.2]{pics/taxonomyBuilder.jpg}
    \caption{Taxonomy Builder input and output}
   \label{fig:taxonomyBuilder}  
\end{figure*}

Given the assumption that there is a granularity and hierarchy of topics among the clusters found in the clusterer step, the taxonomy builder deals with  how to find the hierarchy, and to get a tree structure out of the sets of clusters (Figure ~\ref{fig:taxonomyBuilder} ).\\
\\
The herarchy is found by creating a tree, in this tree, the tree nodes correspond to the clusters found in the clusterer step, the problem is then finding the arcs, for which the authors proposed the following greedy algorithm, there is an arc from cluster $A$ to cluster $B$ only if:
\begin{itemize}
	\item  There is at least one document in common between $A$ and $B$
	\item  $B$ was found during an iteration with higher $K$ in the clusterer step. That is, the topic granularity of Cluster $B$ is in principle higher whereas Cluster $A$'s topic is more general
\end{itemize}


\subsection{Model Builder}
The Model Builder enriches the given taxonomy by adding extra information that might result useful for the 
agents of the call center.\\
The Model Builder adds and orders the following information:
\begin{itemize}
	\item Identify the most common answers and questions
	\item Creates call statistics (i.e: what is the average duration for a call regarding a topic)
	\item Style of the agents,how do the agents open and end conversations
	\item Typical Actions
\end{itemize}

Regarding the common answers and questions, they are arranged and merged for each node by:
\begin{itemize}
	\item Tiling:  Questions and Answers which share a ratio of similar words are gathered so that they appear as a single
				  Entry. This should be useful for the agents when browsing a node in the taxonomy
	\item Ordering: Questions and Answers are ordered according to their appearance in the conversation
\end{itemize}


For the reviewer there is space for improvement for the tiling, one could for example go into a very sophisticated system that tries to capture paraphrases, given that the transcriptions are conversations one could use discourse information to gather paraphrases as proposed by Regneri and Wang in ~\cite{pub6382}

\subsubsection*{Typical Actions}
The typical actions for each node are also gathered, that is the typical instructions that the agent should give the client.
The authors of the paper hypotheses that the actions are usually around the features of the product. So if they know that a feature is the "format button", then they know a verb around that noun will most likely  be an action.

From the reviewer perspective it was not clear in the paper how they actually solved this problem, did they manually gathered the domain lexicon? or they used any external method for gathering this from other sources?.\\
This could include many multi-word expression which are usually hard to handle by the systems.\\
As an important remark, none of this was in somehow assessed by the evaluation they proposed, so how did they assure the quality of this steps is a  good question.


\subsubsection*{Questions and Answers}
Since their input is raw text, the authors mention that in order to capture the questions the use trigger words such as "how", "what","where" which should mark the beginning of a question.\\
Regarding to the answer, it would be the sentence following the question sentence.

In the opinion of the reviewer here is where the noise of the ASP may actually affect the system, it won't affect the creation of the taxonomy, nor the performance of the task from which the authors evaluated their system, but rather it will affect the users of the taxonomy. Very noisy transcriptions will generate wrong questions and propose wrong answers, furthermore it might overlook many questions and therefore potential answers.\\
This issue could not be evaluated by the method they proposed in the paper since again, they use a bag of words approach.

\begin{center}
\begin{table*}[ht]
{\small
\hfill{}
\begin{tabular}{| l | c | c | c | r | }
\hline
 \textbf{System} & \textbf{Extraction} & \textbf{Analysis} & \textbf{Generation} & \textbf{Validation} \\ \hline

\hline
 $TERMINAE$ &  \pbox[t]{20cm}{NLP Tools are used. \\Human intervention is possible} & \pbox[t]{100mm}{Manual\\Similarity of Concepts} & No standard & Only Human \\ \hline
    $SALT$ & \pbox[t]{100mm}{NLP Tools\\ fully automatic} & \pbox[t]{100mm}{Automatic\\ Similarity of Concepts} & No standard & \pbox[t]{20cm}{Partial Human \\ Intervention}\\ \hline
     \pbox[t]{100mm}{ $Learning$ $OWL$ \\$Ontology$ \\$from Text$}&  \pbox[t]{100mm}{ NLP Tools\\Wordnet\\fully automatic} & Not Given & OWL & \pbox[t]{20cm}{Not Given}\\ \hline
     \pbox[t]{100mm}{Reviewed \\ System} & NLP Tools fully automatic & \pbox[t]{20cm}{Text Clustering, \\Document Similarity}  & No Standard & Not Given\\
\hline
\end{tabular}}
\hfill{}
 \caption{Comparison among Systems}
\label{table:comparisonTable}
\end{table*}
\end{center}
\section{Asessing the results}
Since assessing the quality of the generated Taxonomy turned out to be a challenge the
authors proposed the application of realtime topic identification of a call.
The reviewer thinks that the authors wanted to proof the goodness and coverage of the 
taxonomy by using it in a task.\\
\\
The authors ran two experiments.In the first one they tried to identify the topic of a call
given different fractions of the call being observed. In overall one could say that they achieved
good results.
The second experiment consisted in identifying if there were topics harder to identify than others.
As one could expect naturally some topics needed longer fractions of observations to achieve decent results.

\section{Opinion and Comparisons}


The overall construction of the taxonomy given the underlying idea of different granularities of topics seem to work. Nevertheless the reviewer wonders whether the expressiveness and coverage is enough for what it might be real applications that might actually benefit the agents or the users of the callcenter, for example if one were to extend the system into a Question answering System, then this representation will be surely not enough.\\
\\
In ~\cite{DBLP:conf/kdd/LiuSLW12} Liu proposes another method to create a taxonomy from keywords , however they handle different issues which were not addressed by the authors of the reviewed paper, for example fine grain domains and changing domains.\\
Liu proposes using Keywords and not text, since it would provide both an easy way for humans to read the taxonomy and ti will be easier to model how the taxonomy evolves and changes as the domain does.\\
Furthermore, Liu proposes a way to connect the keywords by using text clustering and Information Retrieval, and she addresses many issues not covered by the given paper, this issues are resolved by using information extraction.\\
One of the issues with BoW approach and hierarchical clustering is whether to trust or not the subtyping, for example the concept of 'vehicle insurance" and ''car insurance"  is clear that the later is a sub type of the earlier. Nevertheless from distributional perspective there is no way for the machine to assure to capture it this way, actually the reviewed paper mentions nothing about this, but it might be possible to have this subtype relations inverted given the distribution of the text. Liu on the other hand proposes to use information extraction snowball approach to look for patterns that might allow to capture some of the subtype,supertype relations, thus leaving this task not only to clustering step.\\
\\
A set of interesting remarks are made by  Bedini and Nguyen in ~\cite{artofontology} in their survey about automatic ontology generation they describe the life cycle of ontology generation. Namely they go in a general description of each phase as:
\begin{itemize}
	\item Extraction: The acquisition of Information and the processing of unstructured and semi-structured data
	\item Analysis: finding the relations and concepts int he given information, using heavily NLP techniques
	\item Generation: Translating the relations and concepts into the ontology formalism, as well as merging ontologies.
	\item Validation: A step to check possible wrong relations, usually as the authors mentioned made manually
	\item Evolution: The possibility of an incrementally growing ontology.
\end{itemize}
For the reviewer this life cycle gives an overview of what the authors of the reviewed paper has done, nevertheless the scalability of the proposed method does not leave space for incremental expansion of the taxonomy, that is in order to give evolution to the proposed methods the authors would have to re-run the whole system with the extra information.



\subsection*{Other Systems}

In order to make a comparison of the proposed system in the reviewed paper the table proposed by Sharma using the Life Cycle steps in the previous subsection can be handy. In ~\cite{IJOATsemantic-domain-ontology} Sharma compares two famous systems  $SALT$ and $TERMINAE$ both try to construct ontologies in an automatic way.


One could clearly see in Table~\ref{table:comparisonTable} that some systems admit some human interventions at some points, this could be beneficial since some steps regarding for example Word Sense Disambiguation could be very sensitive to the over all performance of the system.\\
\\
$Terminae$ ~\cite{BiebowSzulman:99} extracts keywords from the given corpora and hand it to an expert, whose task is to select the relevant terms and conceptualize them. Conceptualizing refers to first give a natural language definition of each term and then expressing it in terms of the chosen formalism to represent the ontology.\\
\\
$Salt$ ~\cite{Lonsdale02pepperingknowledge} on the other hand tries to go further by automatizing more parts of the process. Nevertheless it needs other knowledge sources in order to work. it assumes the existance of 3 types of knowledge: a general Knowledge representation of the domain, a dictionary to discover lexical relations among terms and a consistent set of texts. Salt also allows some interaction with the user, in the last stage for example introduces a new step called "Constraint Discovery" in which the user can state things such as "a person only has one birth date". \\
\\
The system proposed in Learning an OWL ontology from text ~\cite{citeulike:136652} is more recent than the previous mentioned ones. It heavily relies on Wordnet for connecting concepts. Nevertheless in the paper is mentioned how they extract the keywords that will be used later on in Wordnet, as a remark also nothing is mentioned about the verification process.\\
\\
It is also worth noticing how the reviewed paper does not describe much about the validation process.
Moreover both the extraction and Analysis part seems to be overlapping for the reviewed system, that could be because they proposed a rather simple structure ( Taxonomy of documents) as instead to a more complicated representation such relations among concepts.\\
A more complicated representation would definitely required a clear separation among those two phases, for example the analysis could required syntactic and Semantic processing to solve anaphoras, in order to be able to capture non-obvious relations given in the texts.

Regarding the question of how to asses Knowledge Resources one interesting research study was done by Mirkin and Dagan in ~\cite{Mirkin:2009:EIU:1609067.1609129} where they compare different Lexical Knowledge Resources such as Wordnet, WikiFS, CBC, Framenet and Snow for the Textual Entailment task.\\
In the Textual entailment task systems are built to predict where a text predicts a hypothesis, usually this requires many preprocessing steps, one of them is extending the hypothesis with extra words which share the same meaning as the ones in the hypothesis.\\
Mirkin and Dagan test different resources and try to answer questions which usually arises in the field of computational linguistics such as: when to use a knowledge resource, and which one to use.\\
This could be an insight that Knowledge Resources and constructions should be more engineered oriented, in terms that they are application oriented.

\section{Conclusions}
To Finalize the reviewer would like to conclude that the reviewed paper is a good attempt to automatically generate Taxonomies, however the way in which the method is assessed and also whether a  taxonomy is enough as a knowledge source, are opened questions.\\
In the final part of this paper the reviewer tried to compared and mentioned other systems which are trying to automatically generate knowledge representation, though they go into a more detailed formalisms they share common tasks and common challenges.\\
In the opinion of the reviewer more development in the NLP area would allow the construction of better Knowledge representations as many of the challenges boils down to sub-nlp tasks such as: Information Extraction, Wordsense desambiguation, Text Entailment, Text Clustering, Relation Extraction.\\
Nevertheless this is a circle since many NLP task would greatly improve their performance by  the automatic extension of knowledge representations.







\bibliography{paper}{}
\bibliographystyle{alpha}






\end{document}


